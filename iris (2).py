# -*- coding: utf-8 -*-
"""iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1filRdh839snf0_RSsC_tpCCraKFJ-FW5
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # data visualisation and plotting
import matplotlib.pyplot as plt # data plotting
import warnings
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

df=pd.read_csv("/content/iris.data")

df.head(5)

df.info()

df.describe()

df['Iris-setosa'].value_counts()

rows, col = df.shape
print("Rows : %s, column : %s" % (rows, col))

g = sns.pairplot(df, hue='Iris-setosa', markers='x')
g = g.map_upper(plt.scatter)
g = g.map_lower(sns.kdeplot)

y= df['Iris-setosa']
X = df.drop('Iris-setosa', axis=1)

X.head(5)

mapping = {
    'Iris-setosa' : 1,
    'Iris-versicolor' : 2,
    'Iris-virginica' : 3
}

X = df.drop(['Iris-setosa'], axis=1).values # Input Feature Values
y = df['Iris-setosa'].replace(mapping).values.reshape(rows,1) # Output values

X = np.hstack(((np.ones((rows,1))), X))# Adding one more column for bias

y.shape

def computeCost(X, y, theta):
    inner_product = (np.dot(X, theta.T) - y) ** 2
    return np.sum(inner_product) / (2 * len(X))

np.random.seed(0) # Let's set the zero for time being
theta = np.random.randn(5,1) # Setting values of theta randomly

print("Theta : %s" % (theta))

theta.shape

theta.ravel()

X.shape

def compute_cost(X, y, theta):
  """
  Compute the cost of a particular choice of theta for linear regression.

  Input Parameters
  ----------------
  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)
      m= number of training examples
      n= number of features (including X_0 column of ones)
  y : 1D array of labels/target value for each traing example. dimension(1 x m)

  theta : 1D array of fitting parameters or weights. Dimension (1 x n)

  Output Parameters
  -----------------
  J : Scalar value.
  """
  predictions = X.dot(theta)
  #print('predictions= ', predictions[:5])
  errors = np.subtract(predictions, y)
  #print('errors= ', errors[:5]) 
  sqrErrors = np.square(errors)
  #print('sqrErrors= ', sqrErrors[:5]) 
  #J = 1 / (2 * m) * np.sum(sqrErrors)
  # OR
  # We can merge 'square' and 'sum' into one by taking the transpose of matrix 'errors' and taking dot product with itself
  # If your confuse about this try to do this with few values for better understanding  
  J = 1/(2 * rows) * errors.T.dot(errors)

  return J

def gradient_descent(X, y, theta, alpha, iterations):
  """
  Compute cost for linear regression.

  Input Parameters
  ----------------
  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)
      m= number of training examples
      n= number of features (including X_0 column of ones)
  y : 1D array of labels/target value for each traing example. dimension(m x 1)
  theta : 1D array of fitting parameters or weights. Dimension (1 x n)
  alpha : Learning rate. Scalar value
  iterations: No of iterations. Scalar value. 

  Output Parameters
  -----------------
  theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)
  cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)
  """
  cost_history = np.zeros(iterations)

  for i in range(iterations):
    predictions = X.dot(theta)
    #print('predictions= ', predictions[:5])
    errors = np.subtract(predictions, y)
    #print('errors= ', errors[:5])
    sum_delta = (alpha / rows) * X.transpose().dot(errors);
    #print('sum_delta= ', sum_delta[:5])
    theta = theta - sum_delta;

    cost_history[i] = compute_cost(X, y, theta)  

    return theta, cost_history

#theta = np.zeros()
iterations = 1000;
alpha = 0.001

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('First 5 values from cost_history =', cost_history[: 5])
print('Last 5 values from cost_history =', cost_history[: -5])

prediction = np.round(np.dot(x_train, theta))

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import mean_absolute_error
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/2,random_state=0)

for i in range(2,10):
    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/2,random_state=42)
    theta, cost_history = gradient_descent(x_train, y_train, theta, alpha, iterations)
    prediction = np.round(np.dot(x_test, theta))
    lin_mae = mean_absolute_error(y_test, prediction)
    print(lin_mae)
    print("cross validation score for %d comparisions: %0.2f (+/- %0.2f)" % (i,lin_mae.mean(), lin_mae.std() * 2))

